\section{Introduction}

\label{intro}

To start on this long way we will focus on the principal key words of this Thesis.

\vspace{5 mm}

\noindent These are: 

\begin{itemize}
\item White Rabbit Project
\begin{itemize}
\item Synchronous Ethernet (SyncE) 
\item PTP (Precision Time Protocol) 
\item EtherBone Core
\item Chronos Project
\end{itemize}
\item SPAD Sensors
\item TSN
\item RISC-V.
\item Neuromorphic Sensors
\end{itemize}

\subsection{White Rabbit Project}

The White Rabbit Project \cite{white-rabbit} is a high-precision synchronization system developed by CERN (the European Organization for Nuclear Research).
It’s designed to achieve extremely accurate time synchronization across \textbf{distributed networks}. 
Specifically, it combines Ethernet and Synchronous Ethernet (SyncE) technologies with PTP (Precision Time Protocol), creating what is known as the \say{White Rabbit Protocol}.
The primary goal is to ensure that all connected devices are synchronized within sub-nanosecond accuracy.

\vspace{5 mm}

\noindent In others words, White Rabbit \cite{WR:ohwr} is a fully deterministic Ethernet-based network for general purpose data transfer and synchronization. 
It can synchronize over 1000 nodes with sub-ns accuracy over fiber lengths of up to 10 km and is commercially available.

\vspace{5 mm}

\noindent According to this ICALPECS 2011 poster (13th International Conference on Accelerator and Large Experimental Physics Control Systems) \cite{white-rabbit:poster}, titled \say{Reliability
in a White Rabbit Network}, White Rabbit (WR) is a time-deterministic, low-latency Ethernet-based network which enables transparent, sub-ns accuracy timing distribution. 
It is being developed to replace the General Machine Timing (GMT) system currently (2011) used at CERN and will become the foundation for the control system of the Facility for Antiproton and Ion Research (FAIR) at GSI.
High reliability is an important issue in WR’s design, since unavailability of the accelerator’s control system will directly translate into expensive downtime of the machine.

\vspace{5 mm}

\noindent Principal characteristics:

\begin{itemize}
\item High Synchronization Precision
    \begin{itemize}
    \item[>] White Rabbit achieves sub-nanosecond precision in synchronizing all nodes within a network, making it essential for particle physics experiments where precise timing is critical.
    \end{itemize}
\item High Time and Frequency Distribution
    \begin{itemize}
    \item[>] White Rabbit enables not only time synchronization but also frequency distribution across devices.
    \item[>] This capability is valuable in scientific experiments, advanced telecommunications, and financial systems where every microsecond matters.
    \end{itemize}
\item Based on Open Standards
    \begin{itemize}
    \item[>] White Rabbit uses open standards, which makes it easy to adopt and adapt to different environments. 
    \item[>] It’s an Open Source hardware and software system, allowing the scientific and tech communities to improve and customize it.
    \end{itemize}
\item Optical Fiber Delay Compensation
    \begin{itemize}
    \item[>] To achieve such high precision, White Rabbit measures and compensates for delays in optical fiber cables, eliminating latency variations that occur over long distances.
    \end{itemize}
\end{itemize}

\vspace{5 mm}

\noindent White Rabbit has been adopted across multiple sectors, including:

\begin{itemize}
\item Scientific Research
    \begin{itemize}
    \item[>] At CERN and other particle physics labs, White Rabbit is crucial for synchronizing detectors, sensors, and other distributed devices in large experimental facilities.
    \end{itemize}
\item Telecommunications
    \begin{itemize}
    \item[>] Telecom providers use White Rabbit to enhance network synchronization accuracy, particularly in \textbf{5G networks} and low-latency services.
    \end{itemize}
\item Industrial Automation
    \begin{itemize}
    \item[>] In production environments, precise synchronization between machines and control systems improves efficiency and reduces errors.
    \end{itemize}
\item Finance
    \begin{itemize}
    \item[>] In finance, where precise timing is critical for high-frequency trading (HFT), White Rabbit enables superior timestamp accuracy.
    \end{itemize}
\end{itemize}

\subsubsection{Synchronous Ethernet (SyncE)}

\subsubsection{PTP (Precision Time Protocol)}

\subsubsection{EtherBone Core}

Etherbone \cite{etherbone-core:ohwr} \cite{etherbone-core:spec} is an FPGA-core that connects Ethernet to internal on-chip wishbone buses permitting any core to talk to any other across Ethernet.

\vspace{5 mm}

\noindent Programmable hardware is increasingly deployed in large, physically distributed control systems. 
Hard real-time systems especially benefit from the determinism and low latency of purpose-built hardware. 
As reconfigurable hardware components replace traditional software-based systems, those hardware components must often communicate directly, over longer distances. 
While traditional protocols like CORBA and SOAP provide an excellent abstraction for software-to-software communication, they are a poor fit for hardware-to-hardware communication. 
Hardware components typically transfer information in read/write bus cycles as opposed to the procedure calling interfaces seen in software.

\vspace{5 mm}

\noindent According to the wiki \cite{etherbone-core:wiki} the Etherbone protocol takes an existing bus standard (Wishbone) and extends this bus to run over the network. 
A concrete bus standard was chosen, because different bus protocols often differ enough that conversion reduces fidelity. 
Wishbone was chosen because it is an open standard, simple, and pipelining. 
The underlying transport protocol is left open, as Etherbone's requirements are easily met. 
This specification defines Etherbone for UDP and TCP.

\paragraph{Architecture}

Etherbone (EB) connects two Wishbone (WB) buses together as shown in the figure below. 
The WB Intercon is a local bus, for example a crossbar interconnect. 
When a WB master wishes to write to a remote slave, it writes to the EB bridge, which is a local WB slave. 
The bridge, acting as an EB master, translates the request into an EB frame and routes it to the receiving EB slave. 
That slave decodes the request and executes the write over Wishbone. 
Finally, the WB interconnect routes the write to the correct slave.

\vspace{5 mm}

\noindent Either or both of the devices may be replaced by software, as shown in the figure below. 
In this scenario, the operating system buffers and sends Etherbone frames as requested by the Etherbone software library.
Client applications may use this library for remote access to any slave attached to an Etherbone equipped wishbone bus. 
This facilitates such tasks as debugging, firmware updates, and monitoring from a dekstop system. 
Applications may also attach devices to a virtual wishbone bus, perhaps to capture bus cycles or trigger an alarm. 
These virtual devices may be mapped into the Wishbone bus of other Etherbone nodes, hardware or software.

\paragraph{Addressing}

Slaves on a Wishbone bus have a mapped address range. Masters read and write to an address on the local bus and the Intercon routes the operation to the matching slave device. 
However, with the introduction of Etherbone, there are now multiple reachable Wishbone buses in the facility-wide system. 
To select the destination slave, additional address information is required.

\vspace{5 mm}

\noindent When using the software interface, an application acquires a handle object for the remote bus. Reads and writes are then performed via the handle object, requiring only the WB bus address per operation. 
To acquire the handle object, the application supplies the hostname and port of the remote WB bus.

\vspace{5 mm}

\noindent For a hardware implementation, the requests come from a local WB master, which can only provide a WB bus address. 
To determine the missing address information, an EB bridge must infer the destination WB bus based only on the local WB address requested. 
To achieve this, the EB bridge establishes a configurable mapping from local WB addresses to destination hostname:ports and target WB addresses.

\vspace{5 mm}

\noindent For example, consider an EB bridge occupying address range \say{0x1000-0x3000} on the local bus. 
There is a remote WB bus available on the the host example.com:3434. 
We would like to access the address range \say{0x100-0x200} on that bus. Thus, we configure our EB bridge to map this range as \say{0x2000-0x2100} on the local bus. 
Now, when a WB write on our local bus to address \say{0x2050} is performed, the bridge transforms this into an EB write destined for example.com:3434 at address \say{0x150}.

\paragraph{Pipelining}
Unlike a local WB bus, where devices answer in a few clock cycles, a remote bus accessed via EB has a much high latency. 
For a 100MHz bus and a distance of only 20km the difference is 10ns to 100us. 
For Internet-scale distances, the latency can easily rise to 100ms.
Therefore, an application which only issues a new read/write operation when the previous operation completes will perform $10^4$ to $10^7$ times
slower over EB than direct WB.

\vspace{5 mm}

\noindent  EB supports pipelining to overcome this significant performance bottleneck. 
Instead of issuing a single operation at a time, an application/device can issue new operations without waiting for the previous operation to complete. 
The results of the operations will arrive in the same order they were issued. 
Whenever new operations do not dependend on still incomplete operations, this can almost entirely mask the performance lost to remote access.

\vspace{5 mm}

\noindent As an example, considering two application using EB. 
The first application is a firmware writing tool that needs to write the firmware and confirm the firmware was written correctly. 
This problem can be readily pipelined; the operations have an order requirement (confirmation happens after write), but the choice of operation to issue
does not depend on previous results. 
The firmware writer can issue a sequence of \say{WWWW...RRRR...} operations in the pipeline without waiting.
Alternatively, it might also use the sequence \say{WRWRWR...} to confirm each word immediately after writing it. 
In both cases, the application can issue all of the operations without waiting. 
This would not be possible if the application were to iterate a remote function. 
Suppose the application wants to compute \say{f(f(f(...f(x)...)))} using a remote WB slave to calculate function f. 
Here, the aplication writes x to the remote slave and reads back \say{y = f(x)}. 
Then the application writes y to the remote slave and reads back \say{z = f(y)}.
The write pattern \say{WRWRWR...} is the same as the firmware loader, but here we can only pipeline a single write-read operation pair together. 
Until we have received the result f(x), we cannot issue f(y).

\vspace{5 mm}

\noindent In Wishbone, several operations can be grouped into a single bus cycle.
A particularly bad situation that can occur is when dependencies appear within a WB cycle. 
Generally, WB cycles acquire the device for use until cycle completion. 
On a local bus, any access pattern will work, as the operations will complete quickly and release the cycle line. 
However, when this happens with EB-sized latencies, a cycle might tie up a device for potentially unacceptable duration. 
Consider for example a WB cycle that reads from one address and writes the result to another address.
Locally, there is no problem; the entire WB cycles executes in a few nanoseconds. 
However, when that same access pattern runs over the network, the slave device needs to wait for the read result to travel to the master and the final write to travel back. 
A very bad design.

\vspace{5 mm}

\noindent Dealing with data dependencies within a cycle is a major complication addressed in the different implementation options described later.

\paragraph{Config Space}

In addition to remote bus access, Etherbone also provides a configuration space. 
This config space is used to specify transmission parameters, recover bus error status codes, and match read results to the requests.

\vspace{5 mm}

\noindent The config space is an complementary 16-bit wide address space attached to every EB slave. 
EB requests can read/write to this configuration space in addition the the normal WB bus. 
The config space is divided up into two regions: the register space and the implementation space. 
All addresses in the register space correspond to EB control registers, specified in this document. 
The register space spans addresses \say{0x0-0x7FFF}. 
The implementation space is guaranteed to be free for whatever use a hardware/software implementation chooses. 
The implementation space spans \say{0x8000-0xFFFF}.

\vspace{5 mm}

\noindent Two important registers in the address space include the error status register, which reports WB error status codes, and the WB device map
pointer, which provides information about the slaves attached to a remote bus. 
The implementation space is typically used by an EB master to receive the data which it read. 
Reads to an EB device trigger a write back to the source EB device. 
Those writebacks are often sent to the implementation space where they can be handled by the EB core/code and invisible to the WB bus.

\paragraph{Bus Widths}

In Wishbone, a bus may have a port width that is \say{8/16/32/64} bits wide.
Thus, a master in one WB bus might write 32-bits at a time, while a slave in another WB bus expects 16-bits at a time. 
Etherbone makes no attempt to convert between differing port widths, because converting a 32-bit write into two 16-bit writes might change semantics.

\vspace{5 mm}

\noindent However, Etherbone does negotiate which port widths are acceptable to both devices. 
This mostly affects software, which can meaningfully support access with different port widths. 
Hardware implementations will typically advertise and accept only one width.

\vspace{5 mm}

\noindent Address spaces in WB are conceptually infinite, but in practice are constrained to a fixed width. 
Address width conversion, as oppposed to port width conversion, is relatively straight-forward. 
Address \say{0x0400} is that same as \say{0x00000400}. 
If a 32-bit device is accessed by a 16-bit device, the 16-bit device can only see the low 16-bits of the larger device's address space.

\vspace{5 mm}

\noindent Address width is negotiated by Etherbone simply to determine the amount of space to reserve for message exchanges. 
A hardware implementation is free to only advertise address widths whose message alignment is convenient to them.

\subsubsection{Chronos Project}

Chronos is an advanced timing and synchronization project at CERN, designed to support extremely precise timekeeping requirements in high-energy physics experiments. 

\vspace{5 mm}

\noindent Built on the foundations of the White Rabbit Project, Chronos \cite{gl:chronos} aims to improve upon CERN’s existing timing systems by achieving even more refined synchronization, essential for coordinating complex experiments and data acquisition across CERN’s infrastructure.

\subsection{SPAD Sensors}

SPAD (Single-Photon Avalanche Diode) \cite{9031298} sensors are an advanced type of photodiode sensor, specifically designed to detect extremely low amounts of light, to the single photon level. 
This makes them ideal for applications requiring exceptional sensitivity and the ability to measure extremely \textbf{fast} light events.

\vspace{5 mm}

\noindent Characteristics and Operation of SPADs:

\begin{itemize}
\item High Sensitivity
    \begin{itemize}
    \item[>] SPAD sensors can detect single photons, thanks to their ability to produce an \say{avalanche} of current when a single photon is detected.
    \item[>] This mechanism allows the sensor to significantly amplify the photon signal, providing a clear and strong response.
    \end{itemize}
\item Fast Response Time
    \begin{itemize}
    \item[>] SPADs are capable of recording events in times on the order of picoseconds.
    \item[>] This is useful in applications that require accurate photon arrival time (known as Time-of-Flight or TOF) measurements, such as in LIDAR and Time-Correlated Single Photon Counting (TCSPC), which is used in fluorescence studies.
    \end{itemize}
\item Geiger Mode Operation
    \begin{itemize}
    \item[>] SPADs operate in \say{Geiger} mode, which means that they are polarized at a voltage above the breakdown voltage. 
    \item[>] This causes an electron avalanche when a photon is detected, which allows the detection of the photon to be clearly recorded.
    \item[>] After detection, the sensor \textbf{needs a reset} to be ready for the next event.
    \end{itemize}
\item Dark noise/dark count noise
    \begin{itemize}
    \item[>] Although SPADs are very sensitive, they can suffer from “dark noise”, i.e. false signals caused by thermal electrons in the absence of light. 
    \item[>] This can be significantly controlled with cooling systems or by using signal processing techniques to filter out false events.
    \end{itemize}
\end{itemize}

\vspace{5 mm}

\noindent SPAD Sensor Applications:

\vspace{5 mm}

\noindent SPAD sensors are useful in many areas because of their their photon-level light detection capability and fast response.

\begin{itemize}
\item LIDAR (Light Detection and Ranging)
    \begin{itemize}
    \item[>] SPADs are an ideal choice for high-precision LIDAR systems that measure distances based on the time of flight of reflected photons. 
    \item[>] This is useful in autonomous vehicles, mapping, and environmental monitoring.
    \end{itemize}
\item Biomedicine and Microscopy
    \begin{itemize}
    \item[>] In fluorescence microscopy, SPADs are used for precise photon detection in molecular dynamics studies, where scientists analyze how molecules interact as a function of time.
    \end{itemize}
\item Time-of-Flight (ToF) cameras
    \begin{itemize}
    \item[>] These cameras, which are used in smartphones and augmented reality devices, employ SPAD sensors to capture \textbf{depth information}.
    \item[>] This allows measuring the distance to each point in the image, creating 3-D maps of the environment.
    \end{itemize}
\item Quantum Communications
    \begin{itemize}
    \item[>] SPADs are essential in quantum communications and quantum cryptography, as they can detect single photons that carry quantum information without loss of the signal integrity.
    \end{itemize}
\end{itemize}

\vspace{5 mm}

\noindent Advantages and Disadvantages

\begin{itemize}
\item Advantages:
    \begin{itemize}
    \item[>] Extreme photon sensitivity, appropriate for low light conditions.
    \item[>] Ultra-fast response time, ideal for precise detection and analysis applications.
    \item[>] Quantum information processing capability, important in cryptography and fundamental science.
    \end{itemize}
\item Disadvantages:
    \begin{itemize}
    \item[>] Dark noise, which can limit its accuracy if not properly controlled.
    \item[>] Cost and complexity of manufacture, making them more expensive than other conventional light sensors.
    \item[>] Need for cooling systems in applications where dark noise is critical, which can complicate system design.
    \end{itemize}
\end{itemize}

\subsection{TSN}

\subsection{RISC-V}

\subsection{Neuromorphic Sensors}

\newpage

\section{ASIC}

The goal of this thesis is to perform this ASIC:

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{figures/ASIC-Scheme.pdf}
    \caption{ASIC to be develop.}
    \label{fig:ASIC}
\end{figure}

\newpage

\section{White Rabbit Node}

See White Rabbit Wiki \cite{WR:wiki}.

\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{figures/WR_node.pdf}
    \caption{WR Node.}
    \label{fig:WR-NODE}
\end{figure}

\newpage

\section{White Rabbit PTP Core}

The WR PTP (Precision Time Protocol) Core (WRPC) \cite{WR-core:wiki} \cite{WR-core:manual} \cite{WRPC:ohwr}, figure \ref{fig:WRPC}, is an Ethernet MAC implementation capable of providing precise timing. 
It can be used for sending and receiving regular Ethernet frames between user-defined HDL modules and a physical medium. 
It also implements the White Rabbit protocol to provide sub-nanosecond time synchronization.

\vspace{5 mm}

\noindent The White Rabbit PTP Core can operate in one of the following modes:
 
\begin{itemize}
\item GrandMaster:
    \begin{itemize}
    \item[>] WR Master synchronized to an external 1-PPS and 10 MHz clock signal, propagates precise timing to other WR-compliant devices.
    \end{itemize}
\item Master:
    \begin{itemize}
    \item[>] WR Master with free-running oscillator, propagates precise timing to other WR-compliant devices.
    \end{itemize}
\item Slave:
    \begin{itemize}
    \item[>] synchronizes its internal oscillator to another WR Master device.
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{figures/WRPC.pdf}
    \caption{WR PTP (Precision Time Protocol) Core (WRPC).}
    \label{fig:WRPC}
\end{figure}

\noindent You can find the HDL description of the WRPC internal components in \cite{WRPC:modules}.

\newpage

\subsection{\textmu RV}

The \textmu RV \cite{urv-core:ohwr} \cite{urv-core:wiki} \cite{Włostowski:2213516} (Micro RISC-V) core is a small-sized implementation of a 32-bit RISC-V core, targeted specifically at FPGAs developed by CERN. 

\begin{figure}[H]
    \centering
    \includegraphics[width=5cm]{figures/urv_logo.png}
    \caption{\textmu RV logo.}
    \label{fig:urv}
\end{figure}

\begin{itemize}
\item Features:
    \begin{itemize}
    \item[>] Supports RV32IM instruction set. 
Division and multiply high instructions are optional and can be emulated to lower the FPGA footprint.
    \item[>] Target: FPGAs.
    \item[>] 4-stage pipeline (FDXW).
    \item[>] All instructions except taken branches/division in one clock cycle.
    \item[>] Code execution from internal memory block.
    \item[>] Wishbone bus (version B.4) for peripheral access.
    \item[>] Simple interrupt handling.
    \item[>] Verilog RTL code.
    \end{itemize}
\end{itemize}

\noindent Since \textmu RV is described in verilog and most of the WRPC component is in VHDL, it will be necessary to use a \textbf{mixed simulator} supported by VUnit.
In this context, one of the best options is \say{QuestaSim}.

\vspace{5mm}

\noindent \href{https://github.com/umarcor}{Umarcor} is planning to develop a container, hosted on our server, for this purpose.

\vspace{5mm}

\noindent \textbf{Task:} Learn how to generate a testbench using both HDLs: VHDL and Verilog.

